{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we have our first ever Kaggle Hack Session! We're going to be competing in the Titantic competition. The goal in this competition is to be able to predict who survived and who passed away during this tragedy, given information about the people involved.\n",
    "\n",
    "We know that getting started for one of these competitions can be difficult, so we've provided this starter notebook to help you get up and running. Let's think about what we need to do when approaching any machine learning competition/problem. \n",
    "\n",
    "1) Determine your problem space. Do you have a classification problem, or a regression problem?\n",
    "\n",
    "2) Determine what model you want to use (Always good to start off with simple models).\n",
    "\n",
    "3) Load in and preprocess your dataset. Examine your database to see if there are any NULL or non-numeric values.\n",
    "\n",
    "4) Split up your dataset into training and testing components. \n",
    "\n",
    "5) Create your model. This entails defining your function, your placeholders, the loss function, and the optimizer. \n",
    "\n",
    "6) Train, evaluate, and iterate on your model!\n",
    "\n",
    "7) Once you have a model that you're satisfied with, load in test.csv (the test set for the Titanic competition), compute your predictions, save them to a CSV file, and submit to Kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the data from the Kaggle website. The direct link is [here](https://www.kaggle.com/c/titanic/data), but we've already downloaded it for you. It's located in the Data subfolder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the Pandas read_csv() function to load in the train.csv\n",
    "titanicTrain = pd.read_csv('Data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the head function to see how the first couple rows of the dataframe looks like\n",
    "titanicTrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId',\n",
       " 'Survived',\n",
       " 'Pclass',\n",
       " 'Name',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Ticket',\n",
       " 'Fare',\n",
       " 'Cabin',\n",
       " 'Embarked']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Figure out what the different column names are\n",
    "titanicTrain.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use other functions such as describe, max, mean, value_counts, etc to learn more about the dataset you're dealing with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the most important parts of any machine learning pipeline. We want to make sure that the inputs we feed into any machine learning model are are valid, non-null, and are numerical values. To get you started with datapreprocessing, we'll show you one example of a column you may want to drop in this dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Visualize the data we're working with\n",
    "titanicTrain.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as you can see above, some of the people don't have values for the age and cabin attributes. There are ways we can deal with this (for example, replace the null values with the median of the other values, replace them with 0, etc), but a simple method is to just drop the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the column\n",
    "titanicTrain.drop(['Cabin'], axis = 1, inplace = True)\n",
    "# alternatively, if you don't wish to modify the original data structure, you can re-assign the result of.drop().\n",
    "#titanicTrain_dropped = titanicTrain.drop(['Cabin'], axis = 1) # For axis number (0 for rows and 1 for columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another column that needs processing is the age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the preprocessing\n",
    "titanicTrain.drop(['Cabin'], axis = 1, inplace = True)\n",
    "medianAge = titanicTrain['Age'].median()\n",
    "embarkedFill = 'S'\n",
    "titanicTrain['Age'] = titanicTrain['Age'].fillna(medianAge, inplace = False)\n",
    "titanicTrain['Embarked'] = titanicTrain['Embarked'].fillna(embarkedFill, inplace = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try it on your own! The functions you will probably be using are (although you're not limited to just these!):\n",
    "- [drop()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html)\n",
    "- [fillna](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html)\n",
    "- [get_dummies()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "- [dropna()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    0\n",
       "Survived       0\n",
       "Pclass         0\n",
       "Sex            0\n",
       "Age            0\n",
       "SibSp          0\n",
       "Parch          0\n",
       "Fare           0\n",
       "Embarked       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Find the other attributes that may give us trouble later on! Once you find these\n",
    "# columns, figure out if you just want to drop the attribute altogether or replace with \n",
    "# median, or something else!\n",
    "\n",
    "\n",
    "# Do the preprocessing\n",
    "titanicTrain.drop(['Cabin'], axis = 1, inplace = True)\n",
    "medianAge = titanicTrain['Age'].median()\n",
    "embarkedFill = 'S'\n",
    "titanicTrain['Age'] = titanicTrain['Age'].fillna(medianAge, inplace = False)\n",
    "titanicTrain['Embarked'] = titanicTrain['Embarked'].fillna(embarkedFill, inplace = False)\n",
    "titanicTrain.drop(['Name'], axis = 1, inplace = True)\n",
    "titanicTrain.drop(['Ticket'], axis = 1, inplace = True)\n",
    "titanicTrain.isnull().sum()\n",
    "\n",
    "# HINT: The name attribute is something you may want to look at. We don't want strings in our ML model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know a couple ways of dealing with null values and string values, feel free to be creative! The best way to get a more accurate machine learning model is to understand the best ways to visualize and clean your data! This is one of the most important steps in any ML pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training/Testing Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now that we've made our final changes to our dataframe, we want to convert it into a matrix of numbers. We want our Y Matrix to be filled with binary labels indicating whether the person survived or not. Our X Matrix should contain all of the features that represent each individual.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 9 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Sex            891 non-null object\n",
      "Age            891 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Fare           891 non-null float64\n",
      "Embarked       891 non-null object\n",
      "dtypes: float64(2), int64(5), object(2)\n",
      "memory usage: 62.7+ KB\n"
     ]
    }
   ],
   "source": [
    "titanicTrain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 5)\n",
      "(891, 1)\n"
     ]
    }
   ],
   "source": [
    "# Convert to matrices. \n",
    "# TODO Add/Remove columns as you see fit\n",
    "X = titanicTrain[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']].as_matrix()\n",
    "Y = titanicTrain['Survived'].as_matrix()\n",
    "Y = Y.reshape([Y.shape[0], 1]) # Reshaping from (891,) to (891,1)\n",
    "print (X.shape)\n",
    "print (Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(OPTIONAL) Remember that whenever we have a dataset, it's good practice to seperate the dataset into 2 parts, one that we will use to train the model, and one that we will use to check how our model is doing as a test/validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into xTrain, yTrain, xTest, and yTest. Take the last 100 examples as test\n",
    "numExamples = X.shape[0]\n",
    "numTestExamples = 100\n",
    "\n",
    "xTrain = X[:numExamples - numTestExamples] # xTrain contains the examples 0-791\n",
    "yTrain = Y[:numExamples - numTestExamples] # yTrain contains the labels for examples 0-791\n",
    "xTest = X[numExamples - numTestExamples:] # xTrain contains the examples 792-891\n",
    "yTest = Y[numExamples - numTestExamples:] # yTrain contains the labels for examples 792-891\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of our data loaded in and preprocessed, we can start on creating our model. This component is pretty open ended. You have the freedom to choose whichever model you'd like to create. If you need inspiration, take a look at the code for linear regression and logistic regression in the week2 and week3 folders. A few other reminders:\n",
    "\n",
    "- Think about what types of objects you'll need to create. Placeholders, variables, optimizers, etc\n",
    "- Think back to how we created the linear regression and logistic regression models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 5)\n",
      "(891, 1)\n",
      "Loss: 0.2203943282365799\n",
      "Loss: 0.20686355233192444\n",
      "Loss: 0.20557086169719696\n",
      "Loss: 0.20505806803703308\n",
      "Loss: 0.2047501802444458\n",
      "Loss: 0.20454718172550201\n",
      "Loss: 0.20440861582756042\n",
      "Loss: 0.20431117713451385\n",
      "Loss: 0.20424030721187592\n",
      "Loss: 0.20418676733970642\n",
      "Loss: 0.2041444182395935\n",
      "Loss: 0.20411038398742676\n",
      "Loss: 0.20408117771148682\n",
      "Loss: 0.20405547320842743\n",
      "Loss: 0.204032301902771\n",
      "Loss: 0.20401056110858917\n",
      "Loss: 0.20399032533168793\n",
      "Loss: 0.20397034287452698\n",
      "Loss: 0.2039516270160675\n",
      "Loss: 0.2039330154657364\n",
      "test acc: 0.6131479144096375\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Use the Pandas read_csv() function to load in the train.csv\n",
    "titanicTrain = pd.read_csv('Data/train.csv')\n",
    "\n",
    "# TODO Find the other attributes that may give us trouble later on! Once you find these\n",
    "# columns, figure out if you just want to drop the attribute altogether or replace with \n",
    "# median, or something else!\n",
    "\n",
    "\n",
    "# Do the preprocessing\n",
    "titanicTrain.drop(['Cabin'], axis = 1, inplace = True)\n",
    "medianAge = titanicTrain['Age'].median()\n",
    "titanicTrain['Age'] = titanicTrain['Age'].fillna(medianAge)\n",
    "titanicTrain['Embarked'] = titanicTrain['Embarked'].fillna('S')\n",
    "titanicTrain.drop(['Name'], axis = 1, inplace = True)\n",
    "titanicTrain.drop(['Ticket'], axis = 1, inplace = True)\n",
    "titanicTrain.isnull().sum()\n",
    "\n",
    "\n",
    "# Convert to matrices. \n",
    "# TODO Add/Remove columns as you see fit\n",
    "X = titanicTrain[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']].as_matrix()\n",
    "Y = titanicTrain['Survived'].as_matrix()\n",
    "Y = Y.reshape([Y.shape[0], 1]) # Reshaping from (891,) to (891,1)\n",
    "print (X.shape)\n",
    "print (Y.shape)\n",
    "\n",
    "\n",
    "# Divide into xTrain, yTrain, xTest, and yTest. Take the last 100 examples as test\n",
    "numExamples = X.shape[0]\n",
    "numTestExamples = 100\n",
    "\n",
    "xTrain = X[:numExamples - numTestExamples] # xTrain contains the examples 0-791\n",
    "yTrain = Y[:numExamples - numTestExamples] # yTrain contains the labels for examples 0-791\n",
    "xTest = X[numExamples - numTestExamples:] # xTest contains the examples 792-891\n",
    "yTest = Y[numExamples - numTestExamples:] # yTest contains the labels for examples 792-891\n",
    "\n",
    "\n",
    "# TODO Create your model here\n",
    "lr = .01 # the learning rate\n",
    "batch_size = 128 # the number of examples we will consider per iterations\n",
    "n_epochs = 10000 # the number of iterations we will do\n",
    "\n",
    "# TODO: Create placeholders for X (our features) and Y (our labels)\n",
    "X = tf.placeholder(tf.float32, [None, 5])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# TODO: create Variables for w (our weights) and b (our biases)\n",
    "w = tf.Variable(tf.truncated_normal(shape = [5, 1], stddev=0.01), name = 'w')\n",
    "b = tf.Variable(tf.zeros([1, 5]), name = 'b')\n",
    "\n",
    "logits = tf.matmul(X, w) + b\n",
    "normalized_logits = tf.nn.sigmoid(logits)\n",
    "# TODO: write code to compute the cross_entropy_loss and the mean_squared_loss.\n",
    "# Experiment with both losses: which one performs better? Why might this be?\n",
    "cross_entropy = tf.reduce_mean(tf.nn.sigmoid(logits))\n",
    "mean_squared_loss = tf.reduce_mean(tf.square(Y - normalized_logits))\n",
    "loss = mean_squared_loss\n",
    "# TODO: Create a GradientDescentOptimizer that minimizes our loss. \n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = lr).minimize(loss)\n",
    "\n",
    "# operations that help us monitour our accuracy\n",
    "cp = tf.equal(tf.argmax(logits, axis = 1), tf.cast(Y, dtype = tf.int64))\n",
    "acc = tf.reduce_mean(tf.cast(cp, tf.float32))\n",
    "\n",
    "# TODO: create a global_variables_initializer, launch the graph, and run the optimization step for n_epochs iterations.\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "for i in range(n_epochs):\n",
    "    #batch = MNIST.train.next_batch(batch_size)\n",
    "    sess.run(opt, feed_dict = {X: xTrain, Y: yTrain})\n",
    "    if i % 500 == 0:\n",
    "        l = loss.eval(feed_dict = {X: xTrain, Y: yTrain})\n",
    "        print(\"Loss: {}\".format(l))\n",
    "a = acc.eval(feed_dict = {X: xTrain, Y: yTrain})\n",
    "print(\"test acc: {}\".format(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've created your model by defining your computational graph, you're ready to start training the model. Remember that training model basically means that we want to run our optimizer object over different parts of our training dataset. A few other reminders:\n",
    "- Remember to create a Tensorflow session and initialize all of your variables\n",
    "- Run your optimizer object at every iteration\n",
    "- Keep track of how your model is doing every now and again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, you have a trained model and you're almost ready to submit! We want to now see how our model does on data that it has never seen before. We want to compute our predictions for the test set. We will then submit these predictions to Kaggle in order to see how accurate we are. A few reminders:\n",
    "- Remember that preprocessing you did for the training dataset? You'll need to do that same preprocessing for this test set as well. \n",
    "- No need to initialize variables or anything. Everything is already trained! We just want to compute our predictions for this new set of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Do the same data preprocessing you did for the train set\n",
    "# TODO Compute the predictions for the testing set by evaluating your logits/normalized logits variables\n",
    "# TODO Check that the predictions are the correct dimensionality \n",
    "\n",
    "# Use the Pandas read_csv() function to load in the train.csv\n",
    "titanicTrain = pd.read_csv('Data/test.csv')\n",
    "\n",
    "# TODO Find the other attributes that may give us trouble later on! Once you find these\n",
    "# columns, figure out if you just want to drop the attribute altogether or replace with \n",
    "# median, or something else!\n",
    "\n",
    "\n",
    "# Do the preprocessing\n",
    "titanicTrain.drop(['Cabin'], axis = 1, inplace = True)\n",
    "medianAge = titanicTrain['Age'].median()\n",
    "titanicTrain['Age'] = titanicTrain['Age'].fillna(medianAge)\n",
    "titanicTrain['Embarked'] = titanicTrain['Embarked'].fillna('S')\n",
    "titanicTrain.drop(['Name'], axis = 1, inplace = True)\n",
    "titanicTrain.drop(['Ticket'], axis = 1, inplace = True)\n",
    "titanicTrain.isnull().sum()\n",
    "\n",
    "X = titanicTrain[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']].as_matrix()\n",
    "Y = titanicTrain['Survived'].as_matrix()\n",
    "Y = Y.reshape([Y.shape[0], 1]) # Reshaping from (891,) to (891,1)\n",
    "print (X.shape)\n",
    "print (Y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Kaggle Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very important to be familiar with the exact Kaggle submission format. We basically want to create a CSV file where the first line of the CSV has the column names '' and '' (this will be different from competition to competition). The following lines will be contain the id number for the test as well as the prediction for that example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "firstRow = [['id', 'pred']]\n",
    "with open(\"predictions.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(firstRow)\n",
    "    # TODO write the id number and the predictions you got from the last step!\n",
    "    # HINT: Using a for loop might be helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the predictions.csv file, you can go ahead and submit to Kaggle! Great job!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
