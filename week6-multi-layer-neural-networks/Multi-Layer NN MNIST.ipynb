{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know everyone is probably tired of MNIST at this point, but let's just see how the accuracy of our multi-layer neural network model compares with the accuracy we got last week with just a single hidden layer NN. \n",
    "\n",
    "**GOAL: The code below creates a single hidden layer neural network. Modify the code so that we have multiple layers instead **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Classic imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want more detailed info on the code below, check out our [full tutorial](https://github.com/uclaacmai/tf-workshop-series-fall17/blob/master/week5-neural-networks/Neural%20Network%20Full%20Tutorial.ipynb) on it. We're basically just creating the structure for a one hidden layer neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "x = tf.placeholder(tf.float32, shape = [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape = [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learningRate = 0.1\n",
    "numHiddenUnits = 50\n",
    "numIterations = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Modify this block of code (and the following block) so that we create a multiple hidden layer NN \n",
    "# instead of an NN with just a single layer\n",
    "\n",
    "# W1 and Bias Variables\n",
    "W1 = tf.Variable(tf.truncated_normal([784, numHiddenUnits], stddev=0.1))\n",
    "B1 = tf.Variable(tf.constant(0.1), [numHiddenUnits])\n",
    "\n",
    "# W2 and Bias Variables\n",
    "W2 = tf.Variable(tf.truncated_normal([numHiddenUnits, 10], stddev=0.1))\n",
    "B2 = tf.Variable(tf.constant(0.1), [10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Intermediate Operations\n",
    "H1 = tf.nn.relu(tf.matmul(x, W1) + B1)\n",
    "yPred = tf.matmul(H1, W2) + B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss Function and Optimizer\n",
    "crossEntropyLoss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = yPred))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learningRate).minimize(crossEntropyLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Help us with visualizing the accuracy as the network trains\n",
    "correctPredictions = tf.equal(tf.argmax(yPred, 1), tf.argmax(y_, 1)) \n",
    "accuracy = tf.reduce_mean(tf.cast(correctPredictions, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global Variable Init\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(init) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, accuracy: 0.07999999821186066, loss: 2.3063743114471436\n",
      "Epoch: 100, accuracy: 0.8999999761581421, loss: 0.46547386050224304\n",
      "Epoch: 200, accuracy: 0.9300000071525574, loss: 0.3149293065071106\n",
      "Epoch: 300, accuracy: 0.9200000166893005, loss: 0.31143081188201904\n",
      "Epoch: 400, accuracy: 0.9200000166893005, loss: 0.24989810585975647\n",
      "Epoch: 500, accuracy: 0.8899999856948853, loss: 0.41437095403671265\n",
      "Epoch: 600, accuracy: 0.8799999952316284, loss: 0.35832229256629944\n",
      "Epoch: 700, accuracy: 0.9100000262260437, loss: 0.33652615547180176\n",
      "Epoch: 800, accuracy: 0.9200000166893005, loss: 0.22202478349208832\n",
      "Epoch: 900, accuracy: 0.9399999976158142, loss: 0.19409088790416718\n",
      "Epoch: 1000, accuracy: 0.9100000262260437, loss: 0.28079578280448914\n",
      "Epoch: 1100, accuracy: 0.9200000166893005, loss: 0.2564100921154022\n",
      "Epoch: 1200, accuracy: 0.9399999976158142, loss: 0.17536747455596924\n",
      "Epoch: 1300, accuracy: 0.949999988079071, loss: 0.175365149974823\n",
      "Epoch: 1400, accuracy: 0.9599999785423279, loss: 0.19462835788726807\n",
      "Epoch: 1500, accuracy: 0.9900000095367432, loss: 0.10126804560422897\n",
      "Epoch: 1600, accuracy: 0.9700000286102295, loss: 0.1497824788093567\n",
      "Epoch: 1700, accuracy: 0.9100000262260437, loss: 0.2621693015098572\n",
      "Epoch: 1800, accuracy: 0.9300000071525574, loss: 0.22341583669185638\n",
      "Epoch: 1900, accuracy: 0.9800000190734863, loss: 0.13378626108169556\n",
      "Epoch: 2000, accuracy: 0.9300000071525574, loss: 0.15296968817710876\n",
      "Epoch: 2100, accuracy: 0.9399999976158142, loss: 0.15631011128425598\n",
      "Epoch: 2200, accuracy: 0.949999988079071, loss: 0.19125951826572418\n",
      "Epoch: 2300, accuracy: 0.9700000286102295, loss: 0.160881906747818\n",
      "Epoch: 2400, accuracy: 0.9700000286102295, loss: 0.15199445188045502\n",
      "Epoch: 2500, accuracy: 0.9300000071525574, loss: 0.2281988561153412\n",
      "Epoch: 2600, accuracy: 0.9900000095367432, loss: 0.07557907700538635\n",
      "Epoch: 2700, accuracy: 0.949999988079071, loss: 0.14024552702903748\n",
      "Epoch: 2800, accuracy: 0.9100000262260437, loss: 0.21873901784420013\n",
      "Epoch: 2900, accuracy: 0.9800000190734863, loss: 0.09403917193412781\n",
      "Epoch: 3000, accuracy: 0.9599999785423279, loss: 0.14981594681739807\n",
      "Epoch: 3100, accuracy: 0.9700000286102295, loss: 0.16664834320545197\n",
      "Epoch: 3200, accuracy: 0.9100000262260437, loss: 0.2685898542404175\n",
      "Epoch: 3300, accuracy: 0.9300000071525574, loss: 0.2574765682220459\n",
      "Epoch: 3400, accuracy: 0.9800000190734863, loss: 0.06280147284269333\n",
      "Epoch: 3500, accuracy: 0.949999988079071, loss: 0.1817607432603836\n",
      "Epoch: 3600, accuracy: 0.9599999785423279, loss: 0.11669956147670746\n",
      "Epoch: 3700, accuracy: 0.9700000286102295, loss: 0.1376563310623169\n",
      "Epoch: 3800, accuracy: 0.9900000095367432, loss: 0.03597499057650566\n",
      "Epoch: 3900, accuracy: 0.9599999785423279, loss: 0.18076537549495697\n",
      "Epoch: 4000, accuracy: 0.949999988079071, loss: 0.14021609723567963\n",
      "Epoch: 4100, accuracy: 1.0, loss: 0.028678791597485542\n",
      "Epoch: 4200, accuracy: 0.9700000286102295, loss: 0.10440553724765778\n",
      "Epoch: 4300, accuracy: 0.9800000190734863, loss: 0.08969789743423462\n",
      "Epoch: 4400, accuracy: 1.0, loss: 0.04524103179574013\n",
      "Epoch: 4500, accuracy: 0.9900000095367432, loss: 0.06591955572366714\n",
      "Epoch: 4600, accuracy: 0.9800000190734863, loss: 0.10764570534229279\n",
      "Epoch: 4700, accuracy: 0.9700000286102295, loss: 0.07646330446004868\n",
      "Epoch: 4800, accuracy: 0.949999988079071, loss: 0.16436904668807983\n",
      "Epoch: 4900, accuracy: 0.9700000286102295, loss: 0.14645642042160034\n",
      "testing accuracy: 0.9610000252723694\n"
     ]
    }
   ],
   "source": [
    "for i in range(numIterations):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    optimizer.run(feed_dict = {x: batch[0], y_: batch[1]})\n",
    "    # every 100 iterations, print out the accuracy\n",
    "    if i % 100 == 0:\n",
    "        # accuracy and loss are both functions that take (x, y) pairs as input, and run a forward pass through the network to obtain a prediction, and then compares the prediction with the actual y.\n",
    "        acc = accuracy.eval(feed_dict = {x: batch[0], y_: batch[1]})\n",
    "        loss = crossEntropyLoss.eval(feed_dict = {x: batch[0], y_: batch[1]})\n",
    "        print(\"Epoch: {}, accuracy: {}, loss: {}\".format(i, acc, loss))\n",
    "\n",
    "# evaluate our testing accuracy       \n",
    "acc = accuracy.eval(feed_dict = {x: mnist.test.images, y_: mnist.test.labels})\n",
    "print(\"testing accuracy: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
