{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this tutorial, we'll be walking through the Tensorflow code behind creating a convolutional neural network. Understanding the code and concepts will require familiarity in creating neural networks with Tensorflow. If you want to review or learn about that, the notes from last week's workshop are [here](https://github.com/uclaacmai/tf-workshop-series/tree/master/week6-neural-nets).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's definite some standard hyperparameters for our network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 2000\n",
    "minibatch_size = 50\n",
    "lr = 1e-4\n",
    "keep = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, we'll create the standard placeholders for our input training examples and corresponding labels. The ```None``` in the first dimension (that denotes the number of examples being fed into the network) allows us to vary the magnitude of the first dimension, which allows us to feed in different batch sizes into our network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape = [None, 28,28,1]) #shape in CNNs is always None x height x width x color channels\n",
    "y_ = tf.placeholder(tf.float32, shape = [None, 10]) #shape is always None x number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Convolutions and max-pooling\n",
    "\n",
    "At the core of convolutional neural networks is the key idea of convolutions. A convolution is a mathematical operation on two functions that produces a third function. Convolutions have rigorous mathematical theory behind them, and if you're interested in learning about that, we recommend Wikipedia's post on convolutions and Christopher Olah's explanation of convolutions. \n",
    "\n",
    "Each convolution operation is followed by a non-linearity, after which a `max-pooling` operation is performed. The purpose of the max-pooling operation is to downsample the activations from the convolution step. It reduces overfitting by representing a set of activations as only the \"most important\" (in a sense) activation in that region. It also reduces computational cost due to a smaller dimensionality. Finally, max-pooling allows us to make assumptions regarding the presence of important features in certain layers. \n",
    "\n",
    "\n",
    "This just defines some methods to make the function calls a little nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    \"\"\"Initializes weights randomly from a normal distribution\n",
    "    Params: shape: list of dimensionality of the tensor to be initialized\n",
    "    \"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"Initializes the bias term randomly from a normal distribution.\n",
    "    Params: shape: list of dimensionality for the bias term.\n",
    "    \"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    \"\"\"Performs a convolution over a given patch x with some filter W.\n",
    "    Uses a stride of length 1 and SAME padding (padded with zeros at the edges)\n",
    "    Params:\n",
    "    x: tensor: the image to be convolved over\n",
    "    W: the kernel (tensor) with which to convolve.\n",
    "    \"\"\"\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# strides is a length-4 list that specifies the amount to move for each dimension of our input x. \n",
    "# the dimensions correspond to the following (in order): batch_size, length of image, width of image, # of channels in image\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    \"\"\"Performs a max pooling operation over a 2 x 2 region\"\"\"\n",
    "    # ksize: we only want to take the maximum over 1 example and 1 channel. \n",
    "    # the middle elements are 2 x 2 because we want to take maxima over 2 x 2 regions\n",
    "    \n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME') # stride 2 right and 2 down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Model Architecture\n",
    "\n",
    "We will implement a (relatively) simple convolutional neural network model. It'll be composed of two convolutional layers, each of which is followed by a max-pooling operation to reduce dimensionality. \n",
    "\n",
    "We will then follow these two convolutional layers with two fully connected layers, similar to those in the vanilla neural network that we implemented last week. However, we're also going to add an operation known as **dropout**. Dropout is an interesting concept that allows large neural networks to prevent from overfitting on the training dataset. With dropout, we discard a predetermined proportion of a hidden layer's activations, so that they don't contribute anything to what the next layer computes. \n",
    "\n",
    "Dropout has many interesting consequences. First of all, it introduces a degree of per-(hidden) layer sparsity when we are training: at any given layer, a certain proportion of the inputs into that layer will be zero. Moreover, since we sample which neurons in particular that we wish to discard at a per-step granularity, we can think of this as training an ensemble of (correlated) neural networks, since the active neurons at each particular training step are zero. When we predict using our network (and don't use dropout), it's as if we're getting a prediction from an ensemble of neural networks, without having to train an ensemble in the first place (which is much more expensive). \n",
    "\n",
    "A few questions to sanity check your understanding of dropout: \n",
    " 1. Why should we not use dropout in the final layer? \n",
    " 2. Why do we not use dropout when testing the network accuracy on testing datasets? Why is it a bad idea to use in predictions? \n",
    " 3. Which dropout probability corresponds to the most number of ways to have the neurons in a hidden layer active/inactive? (hint: think combinatorics). \n",
    " \n",
    "Finally, we will have a output layer that does a standard matrix multiplication to generate class predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32]) # 5 x 5 kernel, across an image with 1 channel to 32 channels\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64]) # 5 x 5 kernel, across an \"image\" with 32 channels to 64 channels\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024]) # This shape can be determined by plugging in something random * 64, and seing the resulting error. \n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 256])\n",
    "b_fc2 = bias_variable([256])\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "W_fc3 = weight_variable([256, 10])\n",
    "b_fc3 = bias_variable([10])\n",
    "y_out = tf.matmul(h_fc2_drop, W_fc3) + b_fc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = y_out, labels = y_)\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_, axis = 1), tf.argmax(y_out, axis = 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "training accuracy: 0.07999999821186066\n",
      "epoch: 100\n",
      "training accuracy: 0.6000000238418579\n",
      "epoch: 200\n",
      "training accuracy: 0.8600000143051147\n",
      "epoch: 300\n",
      "training accuracy: 0.8600000143051147\n",
      "epoch: 400\n",
      "training accuracy: 0.9399999976158142\n",
      "epoch: 500\n",
      "training accuracy: 0.9599999785423279\n",
      "epoch: 600\n",
      "training accuracy: 0.9800000190734863\n",
      "epoch: 700\n",
      "training accuracy: 0.9800000190734863\n",
      "epoch: 800\n",
      "training accuracy: 1.0\n",
      "epoch: 900\n",
      "training accuracy: 0.8999999761581421\n",
      "epoch: 1000\n",
      "training accuracy: 0.9599999785423279\n",
      "epoch: 1100\n",
      "training accuracy: 0.9599999785423279\n",
      "epoch: 1200\n",
      "training accuracy: 0.9399999976158142\n",
      "epoch: 1300\n",
      "training accuracy: 1.0\n",
      "epoch: 1400\n",
      "training accuracy: 0.9399999976158142\n",
      "epoch: 1500\n",
      "training accuracy: 0.9800000190734863\n",
      "epoch: 1600\n",
      "training accuracy: 0.9800000190734863\n",
      "epoch: 1700\n",
      "training accuracy: 0.9800000190734863\n",
      "epoch: 1800\n",
      "training accuracy: 0.9800000190734863\n",
      "epoch: 1900\n",
      "training accuracy: 0.9399999976158142\n",
      "test accuracy: 0.9678999781608582\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(n_epochs):\n",
    "        batch = mnist.train.next_batch(minibatch_size)\n",
    "        training_inputs = batch[0].reshape([minibatch_size,28,28,1])\n",
    "        training_labels = batch[1]\n",
    "        if i % 100 == 0:\n",
    "            print(\"epoch: {}\".format(i))\n",
    "            train_acc = accuracy.eval(feed_dict = {x: training_inputs, y_: training_labels, keep_prob : 1.0})\n",
    "            print(\"training accuracy: {}\".format(train_acc))\n",
    "        sess.run([train_step], feed_dict = {x: training_inputs, y_: training_labels, keep_prob : keep})\n",
    "    test_inputs = mnist.test.images.reshape([-1,28,28,1])\n",
    "    test_labels = mnist.test.labels   \n",
    "    test_acc = accuracy.eval(feed_dict = {x: test_inputs, y_: test_labels, keep_prob : 1.0})\n",
    "    print(\"test accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* CNN [tutorial](https://www.tensorflow.org/tutorials/deep_cnn) from the Tensorflow docs\n",
    "* Stanford's [course](http://cs231n.github.io/convolutional-networks/) on CNNs\n",
    "* Michael Nielson's [chapter](http://neuralnetworksanddeeplearning.com/chap6.html) on CNNs in his book\n",
    "* Facebook's [video](https://www.facebook.com/Engineering/videos/10154673882797200/ ) on ML and CNNs"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
